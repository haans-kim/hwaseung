import pandas as pd
import numpy as np
from typing import Dict, Any, Optional
import os
from pathlib import Path
import hashlib
from datetime import datetime
import pickle
import logging

class DataService:
    def __init__(self):
        self.upload_dir = Path("uploads")
        self.upload_dir.mkdir(exist_ok=True)
        self.data_dir = Path("data")
        self.data_dir.mkdir(exist_ok=True)
        self.pickle_file = self.data_dir / "current_data.pkl"
        self.current_data: Optional[pd.DataFrame] = None
        self.data_info: Optional[Dict[str, Any]] = None
        
        # ì‹œì‘ì‹œ ê¸°ë³¸ ë°ì´í„° ë¡œë“œ ì‹œë„
        self._load_default_data()
    
    def _load_default_data(self) -> bool:
        """ê¸°ë³¸ ë°ì´í„° ë¡œë“œ (pickle íŒŒì¼ì—ì„œ)"""
        try:
            if self.pickle_file.exists():
                with open(self.pickle_file, 'rb') as f:
                    data_package = pickle.load(f)
                    self.current_data = data_package['data']
                    self.data_info = data_package['info']
                    logging.info(f"Loaded default data from pickle: {self.current_data.shape}")
                    return True
        except Exception as e:
            logging.warning(f"Failed to load default data from pickle: {e}")
        return False
    
    def _save_data_to_pickle(self) -> None:
        """í˜„ì¬ ë°ì´í„°ë¥¼ pickle íŒŒì¼ë¡œ ì €ì¥"""
        try:
            if self.current_data is not None and self.data_info is not None:
                data_package = {
                    'data': self.current_data,
                    'info': self.data_info,
                    'timestamp': datetime.now().isoformat()
                }
                with open(self.pickle_file, 'wb') as f:
                    pickle.dump(data_package, f)
                logging.info(f"Saved data to pickle: {self.current_data.shape}")
        except Exception as e:
            logging.error(f"Failed to save data to pickle: {e}")
    
    def get_default_data_status(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ë°ì´í„° ìƒíƒœ í™•ì¸"""
        return {
            "has_default_data": self.current_data is not None,
            "pickle_exists": self.pickle_file.exists(),
            "data_shape": self.current_data.shape if self.current_data is not None else None,
            "pickle_file_path": str(self.pickle_file)
        }
    
    def save_uploaded_file(self, file_content: bytes, filename: str) -> str:
        """ì—…ë¡œë“œëœ íŒŒì¼ì„ ì €ì¥í•˜ê³  ê²½ë¡œ ë°˜í™˜"""
        # íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        name, ext = os.path.splitext(filename)
        safe_filename = f"{name}_{timestamp}{ext}"
        
        file_path = self.upload_dir / safe_filename
        with open(file_path, "wb") as f:
            f.write(file_content)
        
        return str(file_path)
    
    def load_data_from_file(self, file_path: str) -> Dict[str, Any]:
        """íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ ë° ë¶„ì„"""
        try:
            # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì ì ˆí•œ ë¡œë” ì‚¬ìš©
            if file_path.endswith(('.xlsx', '.xls')):
                df = pd.read_excel(file_path)
            elif file_path.endswith('.csv'):
                df = pd.read_csv(file_path)
            else:
                raise ValueError(f"Unsupported file format: {file_path}")
            
            # ë°ì´í„° ì €ì¥
            self.current_data = df
            
            # ë°ì´í„° ì •ë³´ ìƒì„±
            data_info = self._analyze_dataframe(df)
            self.data_info = data_info
            
            # pickle íŒŒì¼ë¡œ ì €ì¥
            self._save_data_to_pickle()
            
            return data_info
            
        except Exception as e:
            raise Exception(f"Error loading data: {str(e)}")
    
    def _analyze_dataframe(self, df: pd.DataFrame) -> Dict[str, Any]:
        """DataFrame ë¶„ì„ ë° ë©”íƒ€ë°ì´í„° ìƒì„±"""
        # ê¸°ë³¸ í†µê³„
        basic_stats = {
            "shape": df.shape,
            "columns": df.columns.tolist(),
            "dtypes": df.dtypes.astype(str).to_dict(),
            "memory_usage": int(df.memory_usage(deep=True).sum()),  # numpy intë¥¼ Python intë¡œ ë³€í™˜
        }
        
        # ê²°ì¸¡ê°’ ë¶„ì„ (numpy íƒ€ì…ì„ Python íƒ€ì…ìœ¼ë¡œ ë³€í™˜)
        missing_counts = df.isnull().sum()
        missing_analysis = {
            "missing_counts": {k: int(v) for k, v in missing_counts.to_dict().items()},
            "missing_percentages": {k: float(v) for k, v in (missing_counts / len(df) * 100).round(2).to_dict().items()},
            "total_missing": int(missing_counts.sum()),
        }
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ í†µê³„
        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
        numeric_stats = {}
        if numeric_columns:
            describe_dict = df[numeric_columns].describe().to_dict()
            # numpy íƒ€ì…ì„ Python íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            numeric_stats = {
                col: {k: float(v) if not pd.isna(v) else None for k, v in stats.items()}
                for col, stats in describe_dict.items()
            }
        
        # ë²”ì£¼í˜• ì»¬ëŸ¼ ë¶„ì„
        categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()
        categorical_stats = {}
        for col in categorical_columns:
            value_counts = df[col].value_counts().head(5)
            categorical_stats[col] = {
                "unique_count": int(df[col].nunique()),
                "top_values": {k: int(v) for k, v in value_counts.to_dict().items()}
            }
        
        # ìƒ˜í”Œ ë°ì´í„°
        sample_data = df.head(10).fillna("").to_dict(orient="records")
        
        return {
            "basic_stats": basic_stats,
            "missing_analysis": missing_analysis,
            "numeric_stats": numeric_stats,
            "categorical_stats": categorical_stats,
            "sample_data": sample_data,
            "numeric_columns": numeric_columns,
            "categorical_columns": categorical_columns,
        }
    
    def get_sample_data(self, n_rows: int = 100) -> Dict[str, Any]:
        """í˜„ì¬ ë°ì´í„°ì˜ ìƒ˜í”Œ ë°˜í™˜"""
        if self.current_data is None:
            raise ValueError("No data loaded")
        
        sample_df = self.current_data.head(n_rows)
        return {
            "data": sample_df.fillna("").to_dict(orient="records"),
            "shape": sample_df.shape,
            "columns": sample_df.columns.tolist()
        }
    
    def get_data_summary(self) -> Dict[str, Any]:
        """í˜„ì¬ ë°ì´í„°ì˜ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        if self.data_info is None:
            raise ValueError("No data loaded")
        
        return {
            "shape": self.data_info["basic_stats"]["shape"],
            "columns": self.data_info["basic_stats"]["columns"],
            "numeric_columns": self.data_info["numeric_columns"],
            "categorical_columns": self.data_info["categorical_columns"],
            "missing_data_percentage": (
                self.data_info["missing_analysis"]["total_missing"] / 
                (self.data_info["basic_stats"]["shape"][0] * self.data_info["basic_stats"]["shape"][1]) * 100
            ),
            "memory_usage_mb": self.data_info["basic_stats"]["memory_usage"] / 1024 / 1024
        }
    
    def validate_data_for_modeling(self) -> Dict[str, Any]:
        """ëª¨ë¸ë§ì„ ìœ„í•œ ë°ì´í„° ê²€ì¦"""
        if self.current_data is None:
            raise ValueError("No data loaded")
        
        df = self.current_data
        issues = []
        
        # ìµœì†Œ í–‰ ìˆ˜ í™•ì¸
        if len(df) < 10:
            issues.append("ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ (ìµœì†Œ 10í–‰ í•„ìš”)")
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ í™•ì¸
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) < 2:
            issues.append("ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 2ê°œ í•„ìš”)")
        
        # ê²°ì¸¡ê°’ ë¹„ìœ¨ í™•ì¸
        missing_ratio = df.isnull().sum().sum() / (len(df) * len(df.columns))
        if missing_ratio > 0.5:
            issues.append(f"ê²°ì¸¡ê°’ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤ ({missing_ratio:.1%})")
        
        # ì¤‘ë³µ í–‰ í™•ì¸
        duplicates = df.duplicated().sum()
        if duplicates > len(df) * 0.1:
            issues.append(f"ì¤‘ë³µ í–‰ì´ ë§ìŠµë‹ˆë‹¤ ({duplicates}ê°œ)")
        
        return {
            "is_valid": len(issues) == 0,
            "issues": issues,
            "recommendations": self._get_data_recommendations(df)
        }
    
    def _get_data_recommendations(self, df: pd.DataFrame) -> list:
        """ë°ì´í„° ê°œì„  ê¶Œê³ ì‚¬í•­"""
        recommendations = []
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬ ê¶Œê³ 
        missing_cols = df.columns[df.isnull().any()].tolist()
        if missing_cols:
            recommendations.append(f"ê²°ì¸¡ê°’ ì²˜ë¦¬ í•„ìš”: {', '.join(missing_cols[:3])}")
        
        # ì´ìƒì¹˜ ê²€ì¶œ ê¶Œê³ 
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols[:3]:  # ì²˜ìŒ 3ê°œ ì»¬ëŸ¼ë§Œ í™•ì¸
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()
            if outliers > len(df) * 0.05:
                recommendations.append(f"{col} ì»¬ëŸ¼ì— ì´ìƒì¹˜ ê²€í†  í•„ìš”")
        
        return recommendations
    
    def augment_data_with_noise(self, target_size: int = 120, noise_factor: float = 0.01) -> Dict[str, Any]:
        """Target ì»¬ëŸ¼ì„ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ ë°ì´í„°ì— ëŒ€í•´ì„œ 10ë°°ìˆ˜ë¡œ ì¦ê°•"""
        if self.current_data is None:
            raise ValueError("No data loaded for augmentation")
        
        original_df = self.current_data.copy()
        original_size = len(original_df)
        
        # Target ì»¬ëŸ¼ ì‹ë³„ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  targetì´ í¬í•¨ëœ ì»¬ëŸ¼ ì°¾ê¸°)
        target_columns = [col for col in original_df.columns if 'target' in col.lower()]
        
        print(f"ğŸ“Š Data Augmentation (Target ì œì™¸ 10ë°°ìˆ˜):")
        print(f"   - Original size: {original_size}")
        print(f"   - Target columns found: {target_columns}")
        print(f"   - Multiplier: 10")
        print(f"   - Expected result: {original_size * 10}")
        
        augmented_rows = []
        
        # ê° ì›ë³¸ í–‰ì— ëŒ€í•´ 10ë°° ì¦ê°• (Target ì»¬ëŸ¼ ì œì™¸)
        for _, row in original_df.iterrows():
            # ì›ë³¸ í–‰ ì¶”ê°€
            augmented_rows.append(row.to_dict())
            
            # 9ë²ˆ ë³µì œí•˜ë©´ì„œ ë…¸ì´ì¦ˆ ì¶”ê°€ (10ë°°ì´ë¯€ë¡œ ì›ë³¸ 1 + ë³µì œ 9 = 10)
            for i in range(9):
                new_row = row.to_dict()
                
                # Target ì»¬ëŸ¼ê³¼ year ì»¬ëŸ¼ì„ ì œì™¸í•œ ìˆ˜ì¹˜í˜• íŠ¹ì„±ì—ë§Œ ë…¸ì´ì¦ˆ ì¶”ê°€
                for col in original_df.columns:
                    # Target ì»¬ëŸ¼ê³¼ year ì»¬ëŸ¼ì€ ì œì™¸
                    if (col not in target_columns and 
                        col != 'year' and 
                        pd.api.types.is_numeric_dtype(original_df[col])):
                        if pd.notna(new_row[col]) and new_row[col] != 0:
                            # Â±1% ë²”ìœ„ì˜ ë…¸ì´ì¦ˆ
                            noise = np.random.normal(0, abs(new_row[col]) * noise_factor)
                            new_row[col] = new_row[col] + noise
                
                augmented_rows.append(new_row)
        
        # ì¦ê°•ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        augmented_df = pd.DataFrame(augmented_rows)
        
        # ë…„ë„ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš° ì •ë ¬
        year_columns = ['year', 'Year', 'YEAR', 'ë…„ë„', 'ì—°ë„']
        year_col = None
        for col in year_columns:
            if col in augmented_df.columns:
                year_col = col
                break
        
        if year_col:
            augmented_df = augmented_df.sort_values(year_col).reset_index(drop=True)
        
        # ì¦ê°•ëœ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
        self.current_data = augmented_df
        self.data_info = self._analyze_dataframe(augmented_df)
        
        # pickle íŒŒì¼ë¡œ ì €ì¥
        self._save_data_to_pickle()
        
        actual_size = len(augmented_df)
        augmented_rows_count = actual_size - original_size
        
        return {
            "message": f"Data augmented from {original_size} to {actual_size} rows (Target column excluded)",
            "original_size": original_size,
            "augmented_size": actual_size,
            "augmentation_applied": True,
            "multiplier": 10,
            "noise_factor": noise_factor,
            "augmented_rows": augmented_rows_count,
            "target_columns_excluded": target_columns,
            "method": "10x augmentation excluding Target columns"
        }

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
data_service = DataService()